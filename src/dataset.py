#!/usr/bin/python
#-*-coding:utf-8-*-

import jieba
import os
import shutil 
from contextlib import nested

import sys
reload(sys)
sys.setdefaultencoding('utf-8')


class dataset():
	"""
		data preparation class,
		1. text split using jieba
		2. delete same and stop words.
		2. generate dict.
		3. generate data(test and train) vector.
	"""
	def __init__(self):
		pass

	def get_unique_id(self, data_dir):
		"""
			get flie unique id famate as {class_id}_type_{text_id}.txt.
			data_dir is the full path of file
		  	e.g ./training_set/4_tec/4_tec_text/text_2001.txt
			where "training" is type, "4" is file class, and "2001" is text id.
			modify this function to adapt your data dir fomate
		"""
		
		dir_list = data_dir.split("/")
		class_id = dir_list[2].split("_")[0]
		text_id = dir_list[4].split(".")[0]
		type_id = dir_list[1].split("_")[0]
		return class_id + "_" + type_id + "_" + text_id

	def splitwords(self, data_dir, data_type):
		""" 
			split word for all files under data_dir 
			save data as <class_{data_type}_id> <words> in ./{data_type}_file2words.txt,
			where data_type is train, test or cv.
			output: {data_type}.txt includes all map of  file unique id and file words.
		"""
		
		if os.path.exists(data_type+".txt"):
			os.remove(data_type+".txt")
		
		list_dirs = os.walk(data_dir)
		for root, _, files in list_dirs:
			# get all files under data_dir
			for fp in files:
				file_path = os.path.join(root, fp)
				file_id = self.get_unique_id(file_path)
				#split words for f, save in file ./data_type.txt
				with nested(open(file_path), open(data_type+".txt", "a+")) as (f1, f):
					data = f1.read()
					#print data
					seg_list = jieba.cut(data, cut_all=False)
					f2.write(file_id + " " + " ".join(seg_list).replace("\n", " ")+"\n")
		
		print "split word for %s file end." % data_type

	def rm_stopwords(self, file_path, word_dict):
		"""
			rm stop word for {file_path}, stop words save in {word_dict} file.
			file_path: file path of file generated by function splitwords.
						each lines of file is format as <file_unique_id> <file_words>. 
			word_dict: file containing stop words, and every stop words in one line.
			output: file_path which have been removed stop words and overwrite original file.
		"""
		
		#read stop word dict and save in stop_dict
		stop_dict = {}
		with open(word_dict) as d:
			for word in d:
				stop_dict[word.strip("\n")] = 1
		
		# remove tmp file if exists
		if os.path.exists(file_path+".tmp"):
			os.remove(file_path+".tmp")
	
		print "now remove stop words in %s." % file_path
		# read source file and rm stop word for each line.
		with nested(open(file_path), open(file_path+".tmp", "a+"))  as (f1, f2):
			for line in f1:
				tmp_list = [] # save words not in stop dict
				words = line.split()[1:]
				for word in words:
					if word not in stop_dict:
						tmp_list.append(word)
				words_without_stop =  " ".join(tmp_list)
				f2.write(words_without_stop + "\n")
		
		# overwrite origin file with file been removed stop words
		shutil.move(file_path+".tmp", file_path)
		print "stop words in %s has been removed." % file_path

	#TODO
	def tf_idf(self, file_path, save_path="../dict/word_dict.txt"):
		"""
			generate key words dict for text using tf_idf algrithm.
			file_path: file have been removed stop words, each lines 
					   of file is format as <file_unique_id> <file_words>.
			output: word_dict.txt, each line in this file is a word
		"""

		# read words in file_path
		# if words are already in dict, pass
		text_dict = []
		with open(file_path) as f:
			for line in f:
				words = line.split()[1:]
				for word in words:
					if word not in text_dict:
						text_dict.append(word)
						print len(text_dict)
			print "distinct word over."

		# write dict to file on disk
		with open(save_path, "w+") as d:
			word_dict = "\n".join(text_dict)
			d.write(word_dict)
		
		print "gen word dict in %s." % save_path

if __name__ == '__main__':
	data_pre = dataset()
	#data_pre.splitwords("../training_set", "train")
	#data_pre.rm_stopwords("train.txt", "../dict/stop_words_ch.txt")
	#data_pre.gen_dict("train.txt")
